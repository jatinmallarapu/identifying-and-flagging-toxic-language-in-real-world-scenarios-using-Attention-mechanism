# Project Aim:
The project focuses on building a model to detect toxic language in real-world scenarios. Utilizing a large dataset, the goal is to create a robust classification model that can identify toxic comments. The dataset is obtained using Kaggle functions, and key models like CNN-BiLSTM with attention mechanisms are implemented.

# Data Exploration: 
Examines dataset, handles imbalanced classes, and visualizes toxicity labels.

# Text Preprocessing: 
Normalizes text, removes unnecessary elements, and tokenizes comments for further analysis.

# Embedding: 
Utilizes pre-trained GloVe embeddings for words in the comments to enhance model understanding.

# Models Used:
## CNN-BiLSTM Model:
Implements a model combining Convolutional Neural Network (CNN) and Bidirectional Long Short-Term Memory (BiLSTM) with attention mechanisms for multi-class text classification.

# Daily Scenario:
This project can be invaluable in real-world applications, such as social media platforms, online forums, or content moderation systems. By automatically identifying toxic language, it helps maintain a safer and more respectful online environment. Content creators and platform administrators can use this tool to filter and manage user-generated content effectively.

# Conclusion:
The CNN-BiLSTM model with attention mechanisms achieved an impressive accuracy of 99.41% in identifying toxic language. The ROC curve analysis further demonstrates the model's effectiveness. In daily scenarios, this tool provides a proactive approach to content moderation, contributing to a healthier online discourse. The visualizations showcase the model's training and validation performance, indicating its robustness in handling toxic language detection.







